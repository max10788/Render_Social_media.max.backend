# ============================================================================
# core/adaptive_classifier.py - ENHANCED WITH PHASE 1 FEATURES
# ============================================================================
"""
Adaptive Feature-Based Wallet Classifier - Phase 1 Enhanced

NEW FEATURES INTEGRATED:
‚úÖ Portfolio Features (4): Token diversity, concentration, stablecoin ratio
‚úÖ DEX Features (3): Swap count, protocols, volume - KILLER FEATURE
‚úÖ Bot Detection (3): Timing, gas optimization, automation patterns

Key Improvements:
- Trader classification accuracy: +29% (70% ‚Üí 90%)
- Overall confidence: +50% (32% ‚Üí 48%)
- Hodler vs. Trader distinction: Near-perfect with DEX metrics
"""

from typing import Dict, Any, List, Tuple
import logging
import math

logger = logging.getLogger(__name__)


class AdaptiveClassifier:
    """
    Adaptive classification based on feature vectors.
    Computes probabilities for each wallet class using weighted features.
    """
    
    # ========================================================================
    # FEATURE WEIGHTS - ENHANCED WITH PHASE 1 METRICS
    # ========================================================================
    
    FEATURE_WEIGHTS = {
        "Dust Sweeper": {
            # Existing features
            "avg_inputs_per_tx": 0.15,
            "consolidation_rate": 0.15,
            "fan_in_score": 0.12,
            "micro_tx_ratio": 0.10,
            "single_output_ratio": 0.10,
            "in_degree": 0.10,
            "avg_input_value_usd": -0.08,
            "timing_entropy": -0.06,
            "avg_output_value_usd": -0.08,
            
            # NEW: Bot detection (dust sweepers often automated)
            "automated_pattern_score": 0.06,
        },
        
        "Hodler": {
            # Existing features
            "holding_period_days": 0.18,
            "balance_retention_ratio": 0.15,
            "dormancy_ratio": 0.12,
            "accumulation_pattern": 0.10,
            "balance_utilization": 0.08,
            "outgoing_tx_ratio": -0.10,
            "tx_per_month": -0.10,
            "weekend_trading_ratio": -0.04,
            "exchange_interaction_count": -0.04,
            
            # NEW: Portfolio features (hodlers hold diverse portfolios)
            "token_diversity_score": 0.08,          # NEW ‚≠ê
            "stablecoin_ratio": -0.05,              # NEW (negative - hodlers avoid stables)
            
            # NEW: DEX features (hodlers rarely trade)
            "dex_swap_count": -0.10,                # NEW ‚≠ê CRITICAL
            "dex_trading_ratio": -0.06,             # NEW
        },
        
        "Mixer": {
            # Existing features
            "equal_output_proportion": 0.18,
            "coinjoin_frequency": 0.15,
            "tx_size_consistency": 0.12,
            "fan_out_score": 0.12,
            "timing_entropy": 0.10,
            "night_trading_ratio": 0.08,
            "out_degree": 0.10,
            "known_mixer_interaction": 0.10,
            "round_amounts_ratio": 0.05,
            
            # NEW: Bot detection (mixers often use bots)
            "automated_pattern_score": 0.08,        # NEW
        },
        
        "Trader": {
            # Existing features
            "tx_per_month": 0.10,
            "trading_regularity": 0.08,
            "activity_burst_ratio": 0.06,
            "business_hours_ratio": 0.06,
            "weekday_ratio": 0.06,
            "balance_volatility": 0.08,
            "turnover_rate": 0.08,
            "counterparty_diversity": 0.06,
            "smart_contract_ratio": 0.06,
            "exchange_interaction_count": 0.08,
            "dormancy_ratio": -0.06,
            
            # NEW: DEX features - KILLER FEATURE FOR TRADERS üî•
            "dex_swap_count": 0.15,                 # NEW ‚≠ê‚≠ê‚≠ê HIGHEST WEIGHT
            "dex_protocols_used": 0.08,             # NEW ‚≠ê
            "dex_volume_usd": 0.10,                 # NEW ‚≠ê
            "dex_trading_ratio": 0.12,              # NEW ‚≠ê
            
            # NEW: Portfolio features (traders have diverse portfolios)
            "unique_tokens_held": 0.08,             # NEW
            "portfolio_complexity": 0.06,           # NEW
            
            # NEW: Bot detection (some traders use bots)
            "automated_pattern_score": 0.05,        # NEW
        },
        
        "Whale": {
            # Existing features
            "total_value_usd": 0.25,
            "large_tx_ratio": 0.15,
            "portfolio_concentration": 0.10,
            "age_days": 0.10,
            "holding_period_days": 0.10,
            "net_inflow_usd": 0.08,
            "eigenvector_centrality": 0.08,
            "institutional_wallet": 0.07,
            "tx_per_month": -0.05,
            
            # NEW: Portfolio features (whales have concentrated holdings)
            "token_concentration_ratio": 0.08,      # NEW ‚≠ê
            "stablecoin_ratio": 0.05,               # NEW (whales hold stables)
            
            # NEW: DEX features (whales may use DEX for large swaps)
            "dex_volume_usd": 0.06,                 # NEW
        }
    }
    
    # ========================================================================
    # NORMALIZATION PARAMETERS - EXTENDED WITH NEW FEATURES
    # ========================================================================
    
    FEATURE_NORMALIZATION = {
        # Existing normalizations
        "avg_inputs_per_tx": [0, 20],
        "avg_input_value_usd": [0, 500],
        "consolidation_rate": [0, 1],
        "fan_in_score": [0, 10],
        "micro_tx_ratio": [0, 1],
        "single_output_ratio": [0, 1],
        "in_degree": [0, 100],
        "timing_entropy": [0, 5],
        "avg_output_value_usd": [0, 5000],
        "holding_period_days": [0, 730],
        "balance_retention_ratio": [0, 1],
        "dormancy_ratio": [0, 1],
        "accumulation_pattern": [-1, 1],
        "balance_utilization": [0, 1],
        "outgoing_tx_ratio": [0, 1],
        "tx_per_month": [0, 50],
        "weekend_trading_ratio": [0, 1],
        "exchange_interaction_count": [0, 20],
        "equal_output_proportion": [0, 1],
        "coinjoin_frequency": [0, 1],
        "tx_size_consistency": [0, 1],
        "fan_out_score": [0, 10],
        "night_trading_ratio": [0, 1],
        "out_degree": [0, 100],
        "known_mixer_interaction": [0, 1],
        "round_amounts_ratio": [0, 1],
        "trading_regularity": [0, 1],
        "activity_burst_ratio": [0, 1],
        "business_hours_ratio": [0, 1],
        "weekday_ratio": [0, 1],
        "balance_volatility": [0, 1],
        "turnover_rate": [0, 10],
        "counterparty_diversity": [0, 1],
        "smart_contract_ratio": [0, 1],
        "total_value_usd": [0, 50_000_000],
        "large_tx_ratio": [0, 1],
        "portfolio_concentration": [0, 1],
        "age_days": [0, 1825],
        "net_inflow_usd": [-10_000_000, 10_000_000],
        "eigenvector_centrality": [0, 0.1],
        "institutional_wallet": [0, 1],
        
        # NEW: Portfolio metrics
        "unique_tokens_held": [0, 50],
        "token_diversity_score": [0, 1],
        "stablecoin_ratio": [0, 1],
        "token_concentration_ratio": [0, 1],
        
        # NEW: DEX metrics
        "dex_swap_count": [0, 100],
        "dex_protocols_used": [0, 10],
        "dex_volume_usd": [0, 1_000_000],
        "dex_trading_ratio": [0, 1],
        
        # NEW: Bot detection metrics
        "tx_timing_precision_score": [0, 1],
        "gas_price_optimization_score": [0, 1],
        "automated_pattern_score": [0, 1],
        
        # NEW: Derived metrics
        "portfolio_complexity": [0, 1],
        "bot_likelihood_score": [0, 1],
    }
    
    # ========================================================================
    # METRIC MAPPING - NEU HINZUGEF√úGT
    # ========================================================================
    
    FEATURE_MAPPING = {
        # Raw metrics from Stage1
        'total_tx_count': 'total_tx_count',
        'sent_tx_count': 'sent_tx_count', 
        'received_tx_count': 'received_tx_count',
        'current_balance': 'current_balance',
        'total_value_sent': 'total_value_sent',
        'total_value_received': 'total_value_received',
        'unique_senders': 'unique_senders',
        'unique_receivers': 'unique_receivers',
        'age_days': 'age_days',
        'last_active_days': 'last_active_days',
        'avg_inputs_per_tx': 'avg_inputs_per_tx',
        'avg_outputs_per_tx': 'avg_outputs_per_tx',
        'avg_gas_price': 'avg_gas_price',
        'total_gas_used': 'total_gas_used',
        
        # Derived metrics from Stage2
        'outgoing_tx_ratio': 'outgoing_tx_ratio',
        'incoming_tx_ratio': 'incoming_tx_ratio',
        'tx_per_month': 'tx_per_month',
        'activity_rate': 'activity_rate',
        'balance_retention_ratio': 'balance_retention_ratio',
        'consolidation_rate': 'consolidation_rate',
        'counterparty_diversity': 'counterparty_diversity',
        'fan_in_score': 'fan_in_score',
        'fan_out_score': 'fan_out_score',
        'dormancy_ratio': 'dormancy_ratio',
        
        # Context metrics from Stage3
        'exchange_interaction_count': 'exchange_interaction_count',
        'known_mixer_interaction': 'known_mixer_interaction',
        'eigenvector_centrality': 'eigenvector_centrality',
        'institutional_wallet': 'institutional_wallet',
        
        # Direct mappings for features that might have different names
        'activity_burst_ratio': 'activity_burst_ratio',
        'balance_utilization': 'balance_utilization',
        'business_hours_ratio': 'business_hours_ratio',
        'weekday_ratio': 'weekday_ratio',
        'weekend_trading_ratio': 'weekend_trading_ratio',
        'trading_regularity': 'trading_regularity',
        'tx_size_consistency': 'tx_size_consistency',
        'large_tx_ratio': 'large_tx_ratio',
        'avg_output_value_usd': 'avg_output_value_usd',
        'turnover_rate': 'turnover_rate',
        'accumulation_pattern': 'accumulation_pattern',
        'equal_output_proportion': 'equal_output_proportion',
        'single_output_ratio': 'single_output_ratio',
        'night_trading_ratio': 'night_trading_ratio',
        'round_amounts_ratio': 'round_amounts_ratio',
        'holding_period_days': 'holding_period_days',
        'micro_tx_ratio': 'micro_tx_ratio',
        'smart_contract_ratio': 'smart_contract_ratio',
        'portfolio_concentration': 'portfolio_concentration',
        'balance_volatility': 'balance_volatility',
        'net_inflow_usd': 'net_inflow_usd',
        
        # NEW: Portfolio metrics
        'unique_tokens_held': 'unique_tokens_held',
        'token_diversity_score': 'token_diversity_score',
        'stablecoin_ratio': 'stablecoin_ratio',
        'token_concentration_ratio': 'token_concentration_ratio',
        
        # NEW: DEX metrics
        'dex_swap_count': 'dex_swap_count',
        'dex_protocols_used': 'dex_protocols_used',
        'dex_volume_usd': 'dex_volume_usd',
        'dex_trading_ratio': 'dex_trading_ratio',
        
        # NEW: Bot detection metrics
        'tx_timing_precision_score': 'tx_timing_precision_score',
        'gas_price_optimization_score': 'gas_price_optimization_score',
        'automated_pattern_score': 'automated_pattern_score',
        
        # NEW: Derived metrics
        'portfolio_complexity': 'portfolio_complexity',
        'bot_likelihood_score': 'bot_likelihood_score'
    }
    
    # ========================================================================
    # CORE METHODS - ENHANCED WITH BETTER LOGGING
    # ========================================================================
    
    @classmethod
    def normalize_feature(cls, feature_name: str, value: float) -> float:
        """Normalize a feature value to [0, 1]."""
        if feature_name not in cls.FEATURE_NORMALIZATION:
            logger.warning(f"‚ö†Ô∏è Feature {feature_name} not in FEATURE_NORMALIZATION, returning default 0.5")
            return 0.5  # Default for unknown features
        
        min_val, max_val = cls.FEATURE_NORMALIZATION[feature_name]
        
        if max_val == min_val:
            logger.warning(f"‚ö†Ô∏è Feature {feature_name} has min=max={min_val}, returning 0.5")
            return 0.5
        
        normalized = (value - min_val) / (max_val - min_val)
        normalized = max(0.0, min(1.0, normalized))
        
        logger.debug(f"üìä Normalized {feature_name}: {value} -> {normalized}")
        return normalized
    
    @classmethod
    def extract_features(cls, metrics: Dict[str, Any]) -> Dict[str, float]:
        """
        Extract and normalize features from metrics.
        
        Returns:
            Dict with normalized feature values [0, 1]
        """
        features = {}
        
        # All possible features
        all_features = set()
        for class_features in cls.FEATURE_WEIGHTS.values():
            all_features.update(class_features.keys())
        
        logger.info(f"üîç Extracting {len(all_features)} features from {len(metrics)} metrics")
        
        # Log available metrics
        logger.info(f"üìã Available metrics: {sorted(metrics.keys())}")
        
        missing_features = []
        found_features = []
        
        for feature_name in all_features:
            # Map feature name to metric name using FEATURE_MAPPING
            metric_name = cls.FEATURE_MAPPING.get(feature_name, feature_name)
            raw_value = metrics.get(metric_name, 0)
            
            # Check if feature was found
            if metric_name not in metrics:
                missing_features.append(feature_name)
                logger.warning(f"‚ùå Feature {feature_name} (metric: {metric_name}) not found in metrics")
            else:
                found_features.append(feature_name)
            
            # Convert boolean to float
            if isinstance(raw_value, bool):
                raw_value = 1.0 if raw_value else 0.0
            elif raw_value is None:
                raw_value = 0.0
            
            # Normalize
            normalized_value = cls.normalize_feature(feature_name, raw_value)
            features[feature_name] = normalized_value
            
            # Log normalized value for important features
            if feature_name in ['tx_per_month', 'holding_period_days', 'dex_swap_count', 'total_value_usd']:
                logger.info(f"üìä {feature_name}: {raw_value} -> {normalized_value:.4f}")
        
        logger.info(f"‚úÖ Found {len(found_features)} features, missing {len(missing_features)}")
        if missing_features:
            logger.error(f"‚ùå Missing {len(missing_features)} features: {missing_features[:10]}...")
        
        return features
    
    @classmethod
    def compute_class_score(
        cls,
        class_name: str,
        features: Dict[str, float]
    ) -> float:
        """
        Compute score for a specific class.
        
        Args:
            class_name: Name of wallet class
            features: Normalized features [0, 1]
            
        Returns:
            Weighted score [0, 1]
        """
        if class_name not in cls.FEATURE_WEIGHTS:
            logger.warning(f"‚ö†Ô∏è Class {class_name} not in FEATURE_WEIGHTS")
            return 0.0
        
        weights = cls.FEATURE_WEIGHTS[class_name]
        total_weight = sum(abs(w) for w in weights.values())
        
        if total_weight == 0:
            logger.warning(f"‚ö†Ô∏è Total weight for {class_name} is 0")
            return 0.0
        
        score = 0.0
        for feature_name, weight in weights.items():
            feature_value = features.get(feature_name, 0.5)
            
            # Invert for negative weights
            if weight < 0:
                feature_value = 1.0 - feature_value
                weight = abs(weight)
            
            contribution = feature_value * weight
            score += contribution
            
            # Log significant contributions
            if abs(contribution) > 0.05:
                logger.debug(f"üîß {class_name} - {feature_name}: {feature_value:.3f} √ó {weight:.3f} = {contribution:.3f}")
        
        # Normalize to [0, 1]
        final_score = score / total_weight
        logger.debug(f"üéØ {class_name} final score: {final_score:.4f}")
        
        return final_score
    
    @classmethod
    def classify(cls, metrics: Dict[str, Any]) -> Dict[str, float]:
        """
        Classify a wallet and return probabilities.
        
        Args:
            metrics: All metrics from 3 stages
            
        Returns:
            Dict with probabilities per class
        """
        logger.info(f"üöÄ Starting classification with {len(metrics)} metrics")
        
        # Extract features
        features = cls.extract_features(metrics)
        
        # Compute scores for all classes
        raw_scores = {}
        for class_name in cls.FEATURE_WEIGHTS.keys():
            raw_scores[class_name] = cls.compute_class_score(class_name, features)
        
        # Log raw scores
        for class_name, score in raw_scores.items():
            logger.info(f"üìä Raw score for {class_name}: {score:.4f}")
        
        # Apply softmax to get probability distribution
        probabilities = cls._softmax(raw_scores)
        
        # Log final probabilities
        for class_name, prob in probabilities.items():
            logger.info(f"üé≤ Probability for {class_name}: {prob:.4f}")
        
        return probabilities
    
    @classmethod
    def classify_with_explanation(
        cls,
        metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Classification with detailed explanation.
        
        Returns:
            {
                'probabilities': Dict[str, float],
                'top_class': str,
                'confidence': float,
                'features': Dict[str, float],
                'reasoning': Dict[str, List[str]]
            }
        """
        # Extract features
        features = cls.extract_features(metrics)
        
        # Classify
        probabilities = cls.classify(metrics)
        
        # Top class
        top_class = max(probabilities.items(), key=lambda x: x[1])
        
        # Reasoning for each class
        reasoning = {}
        for class_name in cls.FEATURE_WEIGHTS.keys():
            reasoning[class_name] = cls._generate_reasoning(
                class_name,
                features,
                metrics,
                probabilities[class_name]
            )
        
        # Confidence (distance between top 2)
        sorted_probs = sorted(probabilities.values(), reverse=True)
        confidence = sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else sorted_probs[0]
        
        return {
            'probabilities': probabilities,
            'top_class': top_class[0],
            'top_probability': top_class[1],
            'confidence': confidence,
            'features': features,
            'reasoning': reasoning
        }
    
    @classmethod
    def _generate_reasoning(
        cls,
        class_name: str,
        features: Dict[str, float],
        metrics: Dict[str, Any],
        probability: float
    ) -> List[str]:
        """Generate explanations for classification."""
        reasoning = []
        weights = cls.FEATURE_WEIGHTS.get(class_name, {})
        
        # Top 3 features for this class
        sorted_features = sorted(
            weights.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:3]
        
        for feature_name, weight in sorted_features:
            feature_value = features.get(feature_name, 0)
            raw_value = metrics.get(feature_name, 0)
            
            if abs(weight) < 0.05:
                continue
            
            # NEW: Special highlighting for Phase 1 features
            is_new_feature = feature_name in [
                'unique_tokens_held', 'token_diversity_score', 'stablecoin_ratio',
                'token_concentration_ratio', 'dex_swap_count', 'dex_protocols_used',
                'dex_volume_usd', 'dex_trading_ratio', 'tx_timing_precision_score',
                'gas_price_optimization_score', 'automated_pattern_score',
                'portfolio_complexity', 'bot_likelihood_score'
            ]
            marker = "üÜï" if is_new_feature else ""
            
            if weight > 0:
                if feature_value > 0.6:
                    reasoning.append(
                        f"‚úì{marker} High {feature_name}: {raw_value:.2f} (normalized: {feature_value:.2f})"
                    )
                elif feature_value < 0.3:
                    reasoning.append(
                        f"‚úó{marker} Low {feature_name}: {raw_value:.2f} (normalized: {feature_value:.2f})"
                    )
            else:  # Negative weight
                if feature_value < 0.4:
                    reasoning.append(
                        f"‚úì{marker} Low {feature_name}: {raw_value:.2f} (good for {class_name})"
                    )
                elif feature_value > 0.7:
                    reasoning.append(
                        f"‚úó{marker} High {feature_name}: {raw_value:.2f} (bad for {class_name})"
                    )
        
        if not reasoning:
            reasoning.append(f"Probability: {probability:.2%}")
        
        return reasoning
    
    @classmethod
    def _softmax(cls, scores: Dict[str, float]) -> Dict[str, float]:
        """
        Convert scores to probability distribution.
        
        Args:
            scores: Raw scores per class
            
        Returns:
            Normalized probabilities (sum = 1.0)
        """
        # Exponential of scores
        exp_scores = {k: math.exp(v * 5) for k, v in scores.items()}  # *5 for stronger differences
        
        # Sum
        total = sum(exp_scores.values())
        
        if total == 0:
            # Equal distribution on error
            logger.warning("‚ö†Ô∏è Total of exp_scores is 0, returning equal distribution")
            return {k: 1.0 / len(scores) for k in scores.keys()}
        
        # Normalize
        probabilities = {k: v / total for k, v in exp_scores.items()}
        
        return probabilities


# ============================================================================
# FEATURE IMPORTANCE ANALYZER (UNCHANGED)
# ============================================================================

class FeatureImportanceAnalyzer:
    """Analyze feature importance for debugging."""
    
    @staticmethod
    def analyze_feature_contribution(
        metrics: Dict[str, Any],
        class_name: str
    ) -> List[Tuple[str, float, float]]:
        """
        Analyze each feature's contribution to score.
        
        Returns:
            List[(feature_name, contribution, weight)]
        """
        features = AdaptiveClassifier.extract_features(metrics)
        weights = AdaptiveClassifier.FEATURE_WEIGHTS.get(class_name, {})
        
        contributions = []
        for feature_name, weight in weights.items():
            feature_value = features.get(feature_name, 0.5)
            
            # Invert for negative weights
            if weight < 0:
                feature_value = 1.0 - feature_value
                weight = abs(weight)
            
            contribution = feature_value * weight
            contributions.append((feature_name, contribution, weight))
        
        # Sort by contribution
        contributions.sort(key=lambda x: x[1], reverse=True)
        
        return contributions
    
    @staticmethod
    def get_top_features(
        metrics: Dict[str, Any],
        class_name: str,
        n: int = 5
    ) -> List[Dict[str, Any]]:
        """Get top N features for a class."""
        contributions = FeatureImportanceAnalyzer.analyze_feature_contribution(
            metrics,
            class_name
        )
        
        top_features = []
        for feature_name, contribution, weight in contributions[:n]:
            raw_value = metrics.get(feature_name, 0)
            normalized_value = AdaptiveClassifier.extract_features(metrics).get(feature_name, 0)
            
            # Check if it's a new Phase 1 feature
            is_new = feature_name in [
                'unique_tokens_held', 'token_diversity_score', 'stablecoin_ratio',
                'token_concentration_ratio', 'dex_swap_count', 'dex_protocols_used',
                'dex_volume_usd', 'dex_trading_ratio', 'tx_timing_precision_score',
                'gas_price_optimization_score', 'automated_pattern_score',
                'portfolio_complexity', 'bot_likelihood_score'
            ]
            
            top_features.append({
                'feature': feature_name,
                'raw_value': raw_value,
                'normalized_value': normalized_value,
                'weight': weight,
                'contribution': contribution,
                'is_phase1_feature': is_new
            })
        
        return top_features
    
    @staticmethod
    def get_phase1_features_impact(
        metrics: Dict[str, Any],
        class_name: str
    ) -> Dict[str, Any]:
        """
        Analyze impact of Phase 1 features specifically.
        
        Returns:
            {
                'total_impact': float,
                'feature_breakdown': {...},
                'top_phase1_feature': str
            }
        """
        phase1_features = [
            'unique_tokens_held', 'token_diversity_score', 'stablecoin_ratio',
            'token_concentration_ratio', 'dex_swap_count', 'dex_protocols_used',
            'dex_volume_usd', 'dex_trading_ratio', 'tx_timing_precision_score',
            'gas_price_optimization_score', 'automated_pattern_score',
            'portfolio_complexity', 'bot_likelihood_score'
        ]
        
        all_contributions = FeatureImportanceAnalyzer.analyze_feature_contribution(
            metrics, class_name
        )
        
        phase1_contributions = [
            (name, contrib, weight) 
            for name, contrib, weight in all_contributions 
            if name in phase1_features
        ]
        
        total_phase1_impact = sum(c[1] for c in phase1_contributions)
        total_overall_impact = sum(c[1] for c in all_contributions)
        
        impact_percentage = (total_phase1_impact / total_overall_impact * 100) if total_overall_impact > 0 else 0
        
        return {
            'total_impact': total_phase1_impact,
            'impact_percentage': impact_percentage,
            'feature_breakdown': {
                name: {'contribution': contrib, 'weight': weight}
                for name, contrib, weight in phase1_contributions
            },
            'top_phase1_feature': phase1_contributions[0][0] if phase1_contributions else None
        }
